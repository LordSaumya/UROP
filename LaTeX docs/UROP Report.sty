\documentclass[fleqn,10pt]{olplainarticle}
% Use option lineno for line numbers 

\usepackage{bibentry}
\nobibliography*

\title{Data-Driven and Physics-Inspired Machine Learning\\[1ex] \large UROP Report U081420}

\author{Saumya Shah}

\keywords{TK}

\begin{abstract}
    TK
\end{abstract}

\begin{document}
\maketitle

\flushbottom

\thispagestyle{empty}

\tableofcontents

\section{Introduction}
TK

\section{Background}
TK

\section{Related Work}

\begin{itemize}
    \item \bibentry{udrescu2020}
    \begin{itemize}
        \item Existing solutions use genetic algorithms or sparse regression.
        \item This paper uses NNs to find simplifying factors like symmetry or separability in the dataset.
        \item 6 simplifying assumptions/properties are used:
        \begin{itemize}
            \item \emph{Units:} variables have known physical units
            \item \emph{Low-order polynomials:} $f$ is/composed of low-order polynomial(s)
            \item \emph{Composition:} $f$ is composed of a small set of elementary functions
            \item \emph{Smoothness:} $f$ is continuous
            \item \emph{Symmetry:} some variables in $f$ are symmetric
            \item \emph{Separability}: $f$ can be separated into sum/product of variables
        \end{itemize}
        \item Full algorithm (recursive in nature):
        \begin{enumerate}
            \item \emph{Dimensional analysis:} Reduces the number of dimensions and simplifies data  (makes the model depend on as few variables as possible).
            \item \emph{Polynomial fit:} Polynomial coefficients are calculated by solving a system of linear equations and testing if the RMSE is lower than the threshold.
            \item \emph{Brute force:} Expressions of increasing complexity are generated by brute force and tested till the error drops below a certain threshold.
            \item \emph{NN-based transformations:} Tests for translational symmetry, separability, equality of variables, and other transformations like power, log, etcetera
        \end{enumerate}
        \item The key improvement of AI Feynman over Eureqa is its ability to decompose the problem into simpler sub-problems with fewer variables.
    \end{itemize}

    \item \bibentry{Khoo2023.2}
    \begin{itemize}
        \item Physics-inspired symbolic regression methods like AI Feynman leverage properties of $f$ like symmetry and separability.
        \item Four variations of AI Feynman were compared
        \begin{itemize}
            \item No additional bias
            \item Observational bias (replacing angular values with their sines/cosines)
            \item Inductive bias (search space restriction)
            \item Both observational and inductive bias
        \end{itemize}
        \item For experiments 1 and 3, none of the equations on the Pareto frontier matched the orbital equation for Mars.
        \item For experiments 2 and 4, 3 out of 9 equations matched.
        \item Experiment 4 (combining inductive and observational biases) was best suited to rediscover the orbital equation of Mars.
    \end{itemize}

    \item \bibentry{Khoo2023.1}
    \begin{itemize}
        \item This paper extends AI Feynman to discover heliocentricity and planarity of Mars' orbit by adding biases.
        \item 
    \end{itemize}
\end{itemize}

\section{Methodology}
TK

\section{Performance Evaluation}
TK

\section{Conclusion}
TK

\nocite{*}
\bibliography{sample}

\section*{Appendices}
TK

\end{document}